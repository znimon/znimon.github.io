---
title: "CSâ€¯7641: Machine Learning"
date: "2023-05-01 00:00:00 -0600"
tags: [OMSCS, Georgia Tech, Machine Learning]
description: Review and Retrospective
image:
  path: https://res.cloudinary.com/de8dxxflb/image/upload/e_background_removal/f_png/v1745382711/gatech_logo_q46ahl.jpg
  alt: "CSâ€¯7641: Machine Learning"
categories: [GeorgiaTech]
---

**Instructor**: *Charlesâ€¯Isbell & Michaelâ€¯Littman*  
**Semester**: *Springâ€¯2023*  
**Overall Rating**: **6.1â€¯/â€¯10** ğŸ‘

---

âœ… **Pros**

- Covers a wide breadth of classic ML algorithms in one course 
- Four largeâ€‘scale assignments force you understand the algorithms

âŒ **Cons**

- Lectures wander into advanced tangents before establishing the fundamentals, which came acroes as selfâ€‘indulgent and disorganized; the core concepts are often lost in the digressions
- Course materials and starter libraries (e.g., ABAGAIL) are outdated or incomplete, so you spend hours hacking around them
- Assignment specs are vague, yet grading is strict 
- The sheer volume, combined with disorganized and poorly presented content, turns it into an unnecessary slog

---

### ğŸ•’ Time Commitment

Plan on **20â€¯â€“â€¯30â€¯hours per week**. Taking it alongside another moderately heavy course (I paired it with CSâ€¯7646) felt like chugging warm milk on a scorching afternoon; technically doable, but youâ€™ll regret the decision.

---

### ğŸ“ Grade Breakdown

| Component | Weight | Details |
|-----------|--------|---------|
| **Assignments** | **60â€¯%** | A1:Â Supervised Learning (15â€¯%)<br>A2:Â Randomized Optimization (15â€¯%)<br>A3: Unsupervised Learning & Dimensionality Reduction (15â€¯%)<br>A4:Â Reinforcement Learning (15â€¯%) |
| **Final Exam** | **30â€¯%** | Cumulative, closedâ€‘book, proctored via Honorlock |
| **Reading/Writing Quiz** | **5â€¯%** | Unlimited attempts, due Weekâ€¯2 |
| **Hypothesis Quiz** | **5â€¯%** | Due Weekâ€¯2 |
| **Extra Credit** | +1â€¯â€“â€¯2â€¯% | Optional problem set and Ed participation |

---

### âœï¸ Assignments

**A1: Supervised Learning**: Decision trees, regression, and ensemble methods; heavy experiment matrix.  
**A2: Randomized Optimization**: Hillâ€‘climb, simulated annealing, GA, MIMIC; lots of parameter sweeps.  
**A3: Unsupervised Learning**: Clustering, PCA/ICA; evaluate metrics vs. dimensionality reduction.  
**A4: Reinforcement Learning**: Implement Qâ€‘learning on a discrete environment and analyse policy performance.

---

### ğŸ§ª Exam

Single **30%** final exam covering the entire syllabus. Expect theory questions (PAC, VCâ€‘dim, information theory) alongside algorithm mechanics. Formula sheets are not allowed.

---

### ğŸ“š Course Content

**Decision Trees**  
From basic classification/regression splits to ID3, information gain, handling continuous attributes, and limits on expressiveness.

**Regression & Function Approximation**  
Linear and polynomial regression, choosing model order, error metrics, crossâ€‘validation, and the biasâ€“variance tradeâ€‘off.

**Neural Networks**  
Perceptrons, sigmoid units, gradientâ€‘descent training, XOR networks, and early multilayerâ€‘perceptron heuristics.

**Instanceâ€‘Based Learning (kâ€‘NN)**  
Distance metrics, domain weighting, curse of dimensionality, and practical considerations for lazy learners.

**Ensemble Methods (Boosting & Bagging)**  
Weak vs. strong learners, AdaBoost mechanics, biasâ€“variance benefits, and illustrative code walkâ€‘throughs.

**Kernel Methods & Support Vector Machines**  
Optimal separating hyperplanes, soft margins, the kernel trick, and largeâ€‘margin intuition.

**Randomized Optimization**  
Hillâ€‘climbing, simulated annealing, genetic algorithms, and MIMIC for search and hyperâ€‘parameter exploration.

**Clustering & Unsupervised Learning**  
Singleâ€‘linkage, kâ€‘means, Gaussian mixtures via EM, soft clustering, and performance properties.

**Feature Engineering**  
Featureâ€‘selection filters/wrappers, search heuristics, PCA, ICA, alternative transformations, and relevance vs. usefulness.

**Information Theory**  
Entropy, mutual information, KLâ€‘divergence, and coding theory basics applied to ML data representation.

**Bayesian Learning & Inference**  
Bayes rule, NaÃ¯veâ€¯Bayes, belief networks, MDL principle, sampling, and inference algorithms.

**Computational Learning Theory & VC Dimension**  
PAC learning, mistake bounds, hypothesisâ€‘space capacity, and sampleâ€‘complexity results.

**Markov Decision Processes (MDPs)**  
States, actions, rewards, discounting, policy evaluation, and dynamicâ€‘programming solutions.

**Reinforcement Learning**  
Qâ€‘learning, valueâ€‘function approximation, exploration vs. exploitation, and convergence guarantees.

**Game Theory & Multiâ€‘Agent RL**  
Minimax, mixed strategies, repeated games, folk theorems, stochastic games, and learning in multiâ€‘agent settings.


---

### ğŸ’¬ Participation & Interaction

Live office hours are indispensable, they focus on defining assignment expectations and grading details. A study group proved valuable, and while the class Slack channel is mostly chatter, it does surface the occasional gem of useful information.

---

### ğŸ’­ Final Thoughts

CSâ€¯7641 aims to be the OMSCS survey of â€œall things ML,â€ but poor organization, distracting lectures, and incomplete and aging tooling make the course challenging for the wrong reasons. Consider pairing it with a light elective, taking it by itself, or postponing it and hope for an overdue course overhaul.
