---
title: "CS 7641: Machine Learning"
date: "2023-05-01 00:00:00 -0600"
tags: [OMSCS, Georgia Tech, Machine Learning]
description: Review and Retrospective
image:
  path: https://res.cloudinary.com/de8dxxflb/image/upload/e_background_removal/f_png/v1745382711/gatech_logo_q46ahl.jpg
  alt: "CS 7641: Machine Learning"
categories: [GeorgiaTech]
---

**Instructor**: *Charles Isbell & Michael Littman*  
**Semester**: *Spring 2023*  
**Overall Rating**: **6.1 / 10** 👎

---

✅ **Pros**

- Covers a wide breadth of classic ML algorithms in one course 
- Four large‑scale assignments force you understand the algorithms

❌ **Cons**

- Lectures wander into advanced tangents before establishing the fundamentals, which came acroes as self‑indulgent and disorganized; the core concepts are often lost in the digressions
- Course materials and starter libraries (e.g., ABAGAIL) are outdated or incomplete, so you spend hours hacking around them
- Assignment specs are vague, yet grading is strict 
- The sheer volume, combined with disorganized and poorly presented content, turns it into an unnecessary slog

---

### 🕒 Time Commitment

Plan on **20 – 30 hours per week**. Taking it alongside another moderately heavy course (I paired it with CS 7646) felt like chugging warm milk on a scorching afternoon; technically doable, but you’ll regret the decision.

---

### 📝 Grade Breakdown

| Component | Weight | Details |
|-----------|--------|---------|
| **Assignments** | **60 %** | A1: Supervised Learning (15 %)<br>A2: Randomized Optimization (15 %)<br>A3: Unsupervised Learning & Dimensionality Reduction (15 %)<br>A4: Reinforcement Learning (15 %) |
| **Final Exam** | **30 %** | Cumulative, closed‑book, proctored via Honorlock |
| **Reading/Writing Quiz** | **5 %** | Unlimited attempts, due Week 2 |
| **Hypothesis Quiz** | **5 %** | Due Week 2 |
| **Extra Credit** | +1 – 2 % | Optional problem set and Ed participation |

---

### ✍️ Assignments

**A1: Supervised Learning**: Decision trees, regression, and ensemble methods; heavy experiment matrix.  
**A2: Randomized Optimization**: Hill‑climb, simulated annealing, GA, MIMIC; lots of parameter sweeps.  
**A3: Unsupervised Learning**: Clustering, PCA/ICA; evaluate metrics vs. dimensionality reduction.  
**A4: Reinforcement Learning**: Implement Q‑learning on a discrete environment and analyse policy performance.

---

### 🧪 Exam

Single **30%** final exam covering the entire syllabus. Expect theory questions (PAC, VC‑dim, information theory) alongside algorithm mechanics. Formula sheets are not allowed.

---

### 📚 Course Content

**Decision Trees**  
From basic classification/regression splits to ID3, information gain, handling continuous attributes, and limits on expressiveness.

**Regression & Function Approximation**  
Linear and polynomial regression, choosing model order, error metrics, cross‑validation, and the bias–variance trade‑off.

**Neural Networks**  
Perceptrons, sigmoid units, gradient‑descent training, XOR networks, and early multilayer‑perceptron heuristics.

**Instance‑Based Learning (k‑NN)**  
Distance metrics, domain weighting, curse of dimensionality, and practical considerations for lazy learners.

**Ensemble Methods (Boosting & Bagging)**  
Weak vs. strong learners, AdaBoost mechanics, bias–variance benefits, and illustrative code walk‑throughs.

**Kernel Methods & Support Vector Machines**  
Optimal separating hyperplanes, soft margins, the kernel trick, and large‑margin intuition.

**Randomized Optimization**  
Hill‑climbing, simulated annealing, genetic algorithms, and MIMIC for search and hyper‑parameter exploration.

**Clustering & Unsupervised Learning**  
Single‑linkage, k‑means, Gaussian mixtures via EM, soft clustering, and performance properties.

**Feature Engineering**  
Feature‑selection filters/wrappers, search heuristics, PCA, ICA, alternative transformations, and relevance vs. usefulness.

**Information Theory**  
Entropy, mutual information, KL‑divergence, and coding theory basics applied to ML data representation.

**Bayesian Learning & Inference**  
Bayes rule, Naïve Bayes, belief networks, MDL principle, sampling, and inference algorithms.

**Computational Learning Theory & VC Dimension**  
PAC learning, mistake bounds, hypothesis‑space capacity, and sample‑complexity results.

**Markov Decision Processes (MDPs)**  
States, actions, rewards, discounting, policy evaluation, and dynamic‑programming solutions.

**Reinforcement Learning**  
Q‑learning, value‑function approximation, exploration vs. exploitation, and convergence guarantees.

**Game Theory & Multi‑Agent RL**  
Minimax, mixed strategies, repeated games, folk theorems, stochastic games, and learning in multi‑agent settings.


---

### 💬 Participation & Interaction

Live office hours are indispensable, they focus on defining assignment expectations and grading details. A study group proved valuable, and while the class Slack channel is mostly chatter, it does surface the occasional gem of useful information.

---

### 💭 Final Thoughts

CS 7641 aims to be the OMSCS survey of “all things ML,” but poor organization, distracting lectures, and incomplete and aging tooling make the course challenging for the wrong reasons. Consider pairing it with a light elective, taking it by itself, or postponing it and hope for an overdue course overhaul.
