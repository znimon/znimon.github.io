---
title: "CS 7650: Natural Language Processing"
date: "2025-04-17 00:00:00 -0600"
tags: [OMSCS, Georgia Tech, Machine Learning]
description: NLP Review and Retrospective
image:
  path: https://res.cloudinary.com/de8dxxflb/image/upload/w_1200,h_630,c_fill,q_auto,f_auto/v1744918409/gatech_bllv8i.png
  alt: "CS 7650: Natural Language Processing"
categories: [GeorgiaTech]
---

**Instructor**: *Mark Riedl*  
**Semester**: *Spring 2025*  
**Overall Rating**: **9.1/10**

---

 ‚úÖ Pros

- *Lectures by Mark Riedl are generally high-quality ‚Äî clear, well-paced, and thoughtfully structured*
- *Coverage of modern NLP topics*

 ‚ùå Cons

- *Automated homework checks can be finicky*
- *Disconnect between lectures and some assignments*
- *Meta AI lectures are generally lower quality*

---


### üïí Time Commitment

Roughly 6-10 hours per assignment and 3 hours per week on notes, modules, and quizes.

---

### ‚úçÔ∏è Assignments

The course includes six programming assignments in Jupyter Notebook format. Each assignment provides starter code and asks students to complete function implementations or tune specific components. Built-in unit tests provide immediate feedback. If your code passes those tests, you‚Äôll likely receive the same or a very similar score when submitting via Gradescope.

I used a mix of local and Google Colab environments, which worked fine, but in hindsight, sticking with Colab might have been easier since the course assumes you'll use it. I did find that code which ran well locally sometimes needed performance tweaks to pass on Gradescope, as its environment has more limited resources.

The midterm required us to read a recent machine learning research paper and write a brief report answering a structured set of questions. Because those questions are broad enough to apply to almost any paper, the exercise provided an accessible framework that helps students get comfortable with ML literature. It was quick to complete and was my overall favorite assignment in the course.

The final mini-project was a more open-ended challenge where you apply what you‚Äôve learned to build and evaluate a complete NLP model. The accompanying recitation was especially helpful.

---

### üìñ Quizzes

Each module includes a short quiz (usually 2‚Äì4 questions). If you‚Äôve watched the lectures, these are generally straightforward.

---


### üìö Course Content

**Module 1: Introduction to NLP**  
A high-level overview that sets the stage for the rest of the course.

**Module 2: Statistical Foundations**  
A review of core statistical concepts like joint distributions, hidden variables, Bayes' Rule, and conditional probabilities.

**Module 3: Classification Techniques**  
Focuses on classification tools and their relevance to NLP.

**Module 4: Neural Network Foundations**  
Covers core deep learning concepts including RNNs, sequence modeling, encoders/decoders, loss functions, LSTMs, teacher forcing, attention mechanisms, and perplexity. This dense module lays the groundwork for much of the course and is referenced heavily in later assignments.

**Module 5: Document Properties and Embeddings**  
A short but useful module on document-level features and embedding techniques.

**Module 6: Modern Neural Architectures**  
Builds on earlier modules to explore transformer-based models and other modern NLP architectures. I appreciated how the course built enough foundation to make these topics approachable.

**Module 7: Information Retrieval**  
One of the stronger Meta AI modules, it focuses on information retrieval. The evaluation section was especially interesting.

**Module 8: Task-Oriented Dialogue**  
A brief module from Meta AI that touches on dialogue systems, but light on depth overall.

**Module 9: Application Summarization**  
One of the better Meta AI lectures, covering summarization techniques.

**Module 10: Machine Reading**  
A very short overview of machine reading concepts.

**Module 11: Open-Domain Question Answering**  
Covers QA systems but felt disorganized in terms of presentation.

**Module 12: Machine Translation**  
Marked a return to higher-quality lectures, and explored classic and modern translation techniques.

**Module 13: Privacy-Preserving NLP**  
A lightweight module providing a surface-level overview of privacy in NLP.

**Module 14: Responsible AI**  
Concludes the course with a brief discussion on ethics and responsible AI practices.

---

### üí¨ Class Participation & Interaction

Ed Discussion is a useful resource for posting questions and seeing common issues. My main issues came from unit tests that were overly rigid. They sometimes expected very specific implementations, which could be confusing if your alternative approach was still correct.

---

### üéØ Learning Outcomes

My biggest takeaways came from Modules 4, 5, and 6. These covered the core building blocks of modern NLP, including RNNs, sequence-to-sequence models, attention mechanisms, embeddings, and more. These concepts gave me a much stronger understanding of how NLP models work, and they gave me the foundation to explore more advanced topics beyond the course.

